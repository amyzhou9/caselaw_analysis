---
title: "About"
author: "Amy Zhou"
date: "2/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview of Project
I was inspired to do this project because of the coronavirus epidemic. In my memory, this feels like one of the longest news information cycles I have ever seen. Thus, I was curious at what factors keep events in the news longer: do illnesses just stay longer over other events such as economic ones? Furthermore, do some publications care about certain things for longer? Thus I want to scrape the headlines of articles from a variety of news publications search for keywords to track topic in the news.

## Collection of Data
I'm still testing out a few different ways of scraping to see which works the best for me. 

First I tried to use python and beautiful soup to scrape for headlines by selecting for the tags surrounding headlines. For each news publication, this may be different. For example. Wall Street Journal artcile headlines are surrounded by the <h3 class = "WSJTheme-headline...> tag, so searching for this can yield the headlines on a given page. This can easily be customized to automatically paginize. However, some websites may be harder to scrape because of access codes and the difference in website structures used. 

Additionally, for the New York Times, I can use the newyorktimes package and the article search API to search for article metadata.

I also found an api called the news api which I can use to scrape multiple news sites. This is however a little fiddly which I am still trying to figure out. Currently, I have to manually change the page variable to scrape each page of results but I am trying to figure out how to automate this. Also I have to be careful not to get blocked from these sites by perhaps putting a wait on the scraper to slow it down.

With the data, I can then turn the data into csvs to be cleaned. I have thus far scraped the 100 most recent BBC articles and have cleaned it in gather.rmd as an example. 

My data collection code and scrapers can be found in this repo: https://github.com/amyzhou9/gov1005-data

## About the Author
This project was made by Amy Zhou, a first year in Canaday who has a love for criminal justice, law, and news media. 


**Github Repo Link**: https://github.com/amyzhou9/gov1005-final-project